# ===============================================================
# T.1 SETUP
#     - Load packages
#     - Define helper functions
# ===============================================================

library(dplyr)
library(catboost)
library(ranger)
library(irr)        # for kappa2
library(caret)      # for createFolds

t6 <- Sys.time()
# T.1.1 Helper: ordinal MAE (distance in categories)
ordinal_mae <- function(actual, pred) {
  actual_i <- as.integer(actual)
  pred_i   <- as.integer(pred)
  mean(abs(actual_i - pred_i))
}

set.seed(123)

# ===============================================================
# T.2 CROSS-VALIDATED COMPARISON: CATBOOST vs RANDOM FOREST
#     - Only on rows where Q8A_target is observed
# ===============================================================

# T.2.1 Define evaluation dataset and folds
df_eval <- dataset_A_imputed %>%
  dplyr::filter(!is.na(Q8A_target))

Q8A_ord <- as.ordered(df_eval$Q8A_target)
df_eval$Q8A_target <- Q8A_ord
target_levels <- levels(Q8A_ord)

K <- 5
folds <- createFolds(df_eval$Q8A_target,
                     k = K,
                     list = TRUE,
                     returnTrain = FALSE)

# Unified predictor set for BOTH models
# (removes target + any target-derived columns to avoid leakage)
predictor_names <- setdiff(
  names(df_eval),
  c("Q8A_target", "q8a", "Q8A_target_catboost", "Q8A_target_RF")
)

# T.2.2 Storage for CV metrics
cv_results <- data.frame(
  model  = character(),
  fold   = integer(),
  acc    = numeric(),
  ordMAE = numeric(),
  qwk    = numeric(),
  stringsAsFactors = FALSE
)

# T.2.3 K-fold loop: fit CatBoost and RF on same folds
for (k in seq_along(folds)) {
  cat("\n=== Fold", k, "===\n")
  
  test_idx  <- folds[[k]]
  train_idx <- setdiff(seq_len(nrow(df_eval)), test_idx)
  
  train <- df_eval[train_idx, , drop = FALSE]
  test  <- df_eval[test_idx,  , drop = FALSE]
  
  # ------------------------------------------------------------
  # T.2.3.1 CATBOOST MODEL (no leakage, shared predictors)
  # ------------------------------------------------------------
  X_train_A <- train[, predictor_names, drop = FALSE]
  X_test_A  <- test[,  predictor_names, drop = FALSE]
  
  # Convert character columns to factors (CatBoost requirement)
  char_cols_train_A <- sapply(X_train_A, is.character)
  X_train_A[char_cols_train_A] <-
    lapply(X_train_A[char_cols_train_A], factor)
  
  char_cols_test_A <- sapply(X_test_A, is.character)
  X_test_A[char_cols_test_A] <-
    lapply(X_test_A[char_cols_test_A], factor)
  
  # Target: ordered factor -> 0..(num_classes-1)
  y_train_fac <- train$Q8A_target
  y_train_int <- as.integer(y_train_fac) - 1L
  
  y_test_fac <- test$Q8A_target
  # y_test_int not needed for training; used only for metrics outside CatBoost
  
  train_pool_A <- catboost.load_pool(
    data  = X_train_A,
    label = y_train_int
  )
  
  # Test pool WITHOUT being used during training (no leakage from test labels)
  test_pool_A <- catboost.load_pool(
    data = X_test_A
    # label omitted intentionally to keep CatBoost training separate from test labels
  )
  
  params_A <- list(
    loss_function  = "MultiClass",
    eval_metric    = "MultiClass",
    iterations     = 800,
    learning_rate  = 0.05,
    depth          = 6,
    random_seed    = 123,
    logging_level  = "Silent",
    # Overfitting detection disabled here to avoid any dependence on test fold
    thread_count   = parallel::detectCores()
  )
  
  model_A <- catboost.train(
    learn_pool = train_pool_A,
    params     = params_A
  )
  
  pred_prob_A <- catboost.predict(
    model_A,
    test_pool_A,
    prediction_type = "Probability"
  )
  
  class_idx_A <- max.col(pred_prob_A)   # 1..num_classes
  pred_A <- factor(
    target_levels[class_idx_A],
    levels  = target_levels,
    ordered = TRUE
  )
  
  # Metrics for CatBoost
  acc_A    <- mean(pred_A == test$Q8A_target)
  ordMAE_A <- ordinal_mae(test$Q8A_target, pred_A)
  qwk_A    <- kappa2(
    data.frame(truth = test$Q8A_target, pred = pred_A),
    weight = "squared"
  )$value
  
  cv_results <- rbind(
    cv_results,
    data.frame(
      model  = "CatBoost",
      fold   = k,
      acc    = acc_A,
      ordMAE = ordMAE_A,
      qwk    = qwk_A,
      stringsAsFactors = FALSE
    )
  )
  
  # ------------------------------------------------------------
  # T.2.3.2 RANDOM FOREST MODEL (same predictors, no leakage)
  # ------------------------------------------------------------
  rf_formula_B <- as.formula(
    paste("Q8A_target ~", paste(predictor_names, collapse = " + "))
  )
  
  # Use your best config from grid_B:
  # mtry = 41, min.node.size = 50, sample.fraction = 0.6, respect.unordered.factors = "order"
  rf_model_B <- ranger(
    formula                   = rf_formula_B,
    data                      = train,
    num.trees                 = 500,
    mtry                      = 41,
    min.node.size             = 50,
    sample.fraction           = 0.6,
    classification            = TRUE,
    importance                = "impurity_corrected",
    probability               = FALSE,
    num.threads               = parallel::detectCores(),
    respect.unordered.factors = "order"
  )
  
  rf_pred_B <- predict(rf_model_B, data = test)
  pred_B    <- rf_pred_B$predictions
  
  # Ensure ordered factor with same levels
  pred_B <- factor(pred_B, levels = target_levels, ordered = TRUE)
  
  # Metrics for RF
  acc_B    <- mean(pred_B == test$Q8A_target)
  ordMAE_B <- ordinal_mae(test$Q8A_target, pred_B)
  qwk_B    <- kappa2(
    data.frame(truth = test$Q8A_target, pred = pred_B),
    weight = "squared"
  )$value
  
  cv_results <- rbind(
    cv_results,
    data.frame(
      model  = "RandomForest",
      fold   = k,
      acc    = acc_B,
      ordMAE = ordMAE_B,
      qwk    = qwk_B,
      stringsAsFactors = FALSE
    )
  )
}

# T.2.4 Summary comparison of CV metrics
cv_summary <- cv_results %>%
  dplyr::group_by(model) %>%
  dplyr::summarise(
    mean_acc    = mean(acc),
    sd_acc      = sd(acc),
    mean_ordMAE = mean(ordMAE),
    sd_ordMAE   = sd(ordMAE),
    mean_qwk    = mean(qwk),
    sd_qwk      = sd(qwk),
    .groups = "drop"
  )

print(cv_summary)

# ===============================================================
# T.3 AGREEMENT ON IMPUTED VALUES (MISSING Q8A_target)
# ===============================================================

# T.3.1 Basic checks and extraction of imputed parts
stopifnot(nrow(dataset_A_imputed) == nrow(dataset_B_imputed))

no_target <- is.na(dataset_A_imputed$Q8A_target)

catboost_imp <- dataset_A_imputed$Q8A_target_catboost[no_target]
rf_imp       <- dataset_B_imputed$Q8A_target_RF[no_target]

# T.3.2 Agreement rate and cross-tab
agree_rate <- mean(catboost_imp == rf_imp)
cat("Agreement rate on imputed rows:", round(agree_rate, 3), "\n")

tab_diff <- table(CatBoost = catboost_imp, RF = rf_imp)
print(tab_diff)

# ===============================================================
# T.4 UNIFIED RESULT TABLE
#     Combines CV metrics + agreement metrics into one table
# ===============================================================

# Convert cv_summary into nicer column names
cv_summary_clean <- cv_summary %>%
  dplyr::rename(
    acc_mean    = mean_acc,
    acc_sd      = sd_acc,
    ordMAE_mean = mean_ordMAE,
    ordMAE_sd   = sd_ordMAE,
    qwk_mean    = mean_qwk,
    qwk_sd      = sd_qwk
  )

# Flatten agreement table for inclusion
tab_diff_df <- as.data.frame(tab_diff)

# Create unified result list-table
unified_results <- list(
  cross_validation_summary = cv_summary_clean,
  imputation_agreement_rate = data.frame(
    metric = "agreement_rate",
    value  = agree_rate
  ),
  imputation_cross_tab = tab_diff_df
)

# Print unified results nicely
print(unified_results)


t7 <- Sys.time()
t7-t6
